/**
 * Text Chunker Module - Stage 3 of Content Processing Pipeline
 * 
 * Purpose: Break down large cleaned content into smaller, manageable chunks for AI processing
 * Input: Clean, structured text from content cleaner
 * Output: Array of text chunks with metadata for efficient AI processing
 */

interface TextChunk {
  id: string;
  content: string;
  metadata: {
    position: number;
    wordCount: number;
    charCount: number;
    startIndex: number;
    endIndex: number;
    hasOverlap: boolean;
    section?: string;
    chunkingMethod: string;
  };
  source: {
    url: string;
    title: string;
    originalIndex: number;
  };
}

interface ChunkedContent {
  chunks: TextChunk[];
  metadata: {
    totalChunks: number;
    averageChunkSize: number;
    chunkingStrategy: string;
    processingTime: number;
    originalLength: number;
    totalChunkedLength: number;
  };
}

interface TextChunkerInput {
  content: string;
  title: string;
  url: string;
  originalIndex: number;
}

// ========================================
// CHUNKING CONFIGURATION
// ========================================

const CHUNKING_CONFIG = {
  // Target chunk size in words
  targetChunkSize: 800,
  
  // Minimum chunk size (to avoid tiny chunks)
  minChunkSize: 300,
  
  // Maximum chunk size (hard limit)
  maxChunkSize: 1200,
  
  // Overlap size between chunks (words)
  overlapSize: 100,
  
  // Sentence boundary preservation
  preserveSentences: true,
  
  // Paragraph boundary preservation  
  preserveParagraphs: true
};

// ========================================
// STEP 1: TEXT PREPROCESSING
// ========================================

/**
 * Preprocess text for optimal chunking
 * 
 * What this does:
 * - Normalize whitespace and line breaks
 * - Identify section headers and natural break points
 * - Mark paragraph boundaries
 * - Prepare text structure for smart chunking
 */
const preprocessTextForChunking = (content: string): {
  processedText: string;
  sections: Array<{ start: number; end: number; title: string }>;
  paragraphs: Array<{ start: number; end: number }>;
} => {
  console.log("üìù Step 1: Preprocessing text for chunking...");
  
  // Normalize whitespace and line breaks
  let processedText = content
    .replace(/\r\n/g, '\n')
    .replace(/\r/g, '\n')
    .replace(/\n{3,}/g, '\n\n')
    .replace(/[ \t]+/g, ' ')
    .trim();
  
  // Identify section headers (lines that look like titles)
  const sections: Array<{ start: number; end: number; title: string }> = [];
  const lines = processedText.split('\n');
  
  lines.forEach((line, index) => {
    const trimmed = line.trim();
    
    // Detect potential section headers
    if (
      trimmed.length > 0 &&
      trimmed.length < 100 && // Not too long
      /^[A-Z]/.test(trimmed) && // Starts with capital
      !/[.!?]$/.test(trimmed) && // Doesn't end with sentence punctuation
      trimmed.split(' ').length <= 8 && // Not too many words
      !/^(The|A|An|This|That|These|Those|In|On|At|For|With|By)/.test(trimmed) // Not starting with common article/preposition
    ) {
      const start = processedText.indexOf(line);
      if (start !== -1) {
        sections.push({
          start,
          end: start + line.length,
          title: trimmed
        });
      }
    }
  });
  
  // Identify paragraph boundaries
  const paragraphs: Array<{ start: number; end: number }> = [];
  const paragraphTexts = processedText.split(/\n\s*\n/);
  let currentIndex = 0;
  
  paragraphTexts.forEach(paragraph => {
    if (paragraph.trim().length > 0) {
      const start = currentIndex;
      const end = start + paragraph.length;
      paragraphs.push({ start, end });
      currentIndex = end + 2; // Account for paragraph separator
    }
  });
  
  console.log(`‚úÖ Preprocessing complete: ${sections.length} sections, ${paragraphs.length} paragraphs identified`);
  
  return { processedText, sections, paragraphs };
};

// ========================================
// STEP 2: SMART BOUNDARY DETECTION
// ========================================

/**
 * Find optimal chunk boundaries that respect sentence and paragraph structure
 * 
 * What this does:
 * - Find sentence boundaries near target chunk size
 * - Prefer paragraph boundaries when possible
 * - Avoid breaking in the middle of sentences
 * - Consider section headers as natural break points
 */
const findOptimalChunkBoundaries = (
  text: string,
  targetSize: number,
  sections: Array<{ start: number; end: number; title: string }>,
  paragraphs: Array<{ start: number; end: number }>
): number[] => {
  console.log("üéØ Step 2: Finding optimal chunk boundaries...");
  
  const words = text.split(/\s+/);
  const boundaries: number[] = [0]; // Always start at beginning
  
  let currentPosition = 0;
  
  while (currentPosition < words.length) {
    const targetPosition = Math.min(currentPosition + targetSize, words.length);
    
    // If we're at the end, we're done
    if (targetPosition >= words.length) {
      break;
    }
    
    // Find the best boundary near the target position
    let bestBoundary = targetPosition;
    
    // Strategy 1: Look for section boundaries first
    const currentCharIndex = words.slice(0, targetPosition).join(' ').length;
    const nearbySection = sections.find(section => 
      Math.abs(section.start - currentCharIndex) < 200 // Within 200 characters
    );
    
    if (nearbySection) {
      // Convert character index back to word index
      const sectionWordIndex = words.slice(0).findIndex((_, index) => 
        words.slice(0, index + 1).join(' ').length >= nearbySection.start
      );
      if (sectionWordIndex > currentPosition + CHUNKING_CONFIG.minChunkSize) {
        bestBoundary = sectionWordIndex;
        console.log(`üìç Using section boundary: "${nearbySection.title}"`);
      }
    }
    
    // Strategy 2: Look for paragraph boundaries
    if (bestBoundary === targetPosition) {
      const searchStart = Math.max(currentPosition + CHUNKING_CONFIG.minChunkSize, targetPosition - 100);
      const searchEnd = Math.min(targetPosition + 100, words.length);
      
      for (let i = searchEnd; i >= searchStart; i--) {
        const charIndex = words.slice(0, i).join(' ').length;
        const isNearParagraph = paragraphs.some(p => 
          Math.abs(p.end - charIndex) < 50
        );
        
        if (isNearParagraph) {
          bestBoundary = i;
          console.log(`üìç Using paragraph boundary at word ${i}`);
          break;
        }
      }
    }
    
    // Strategy 3: Look for sentence boundaries
    if (bestBoundary === targetPosition) {
      const searchStart = Math.max(currentPosition + CHUNKING_CONFIG.minChunkSize, targetPosition - 50);
      const searchEnd = Math.min(targetPosition + 50, words.length);
      
      for (let i = searchEnd; i >= searchStart; i--) {
        const word = words[i - 1];
        if (word && /[.!?]$/.test(word)) {
          bestBoundary = i;
          console.log(`üìç Using sentence boundary at word ${i}`);
          break;
        }
      }
    }
    
    // Ensure we don't create chunks that are too small or too large
    if (bestBoundary - currentPosition < CHUNKING_CONFIG.minChunkSize) {
      bestBoundary = Math.min(currentPosition + CHUNKING_CONFIG.minChunkSize, words.length);
    } else if (bestBoundary - currentPosition > CHUNKING_CONFIG.maxChunkSize) {
      bestBoundary = currentPosition + CHUNKING_CONFIG.maxChunkSize;
    }
    
    boundaries.push(bestBoundary);
    currentPosition = bestBoundary;
  }
  
  console.log(`‚úÖ Found ${boundaries.length - 1} chunk boundaries`);
  return boundaries;
};

// ========================================
// STEP 3: CHUNK CREATION WITH OVERLAP
// ========================================

/**
 * Create chunks with optional overlap for context preservation
 * 
 * What this does:
 * - Create chunks based on calculated boundaries
 * - Add overlap between chunks to maintain context
 * - Generate unique IDs for each chunk
 * - Calculate metadata for each chunk
 */
const createChunksWithOverlap = (
  text: string,
  boundaries: number[],
  input: TextChunkerInput
): TextChunk[] => {
  console.log("‚úÇÔ∏è Step 3: Creating chunks with overlap...");
  
  const words = text.split(/\s+/);
  const chunks: TextChunk[] = [];
  
  for (let i = 0; i < boundaries.length - 1; i++) {
    const startBoundary = boundaries[i];
    const endBoundary = boundaries[i + 1];
    
    // Calculate overlap
    let chunkStart = startBoundary;
    let chunkEnd = endBoundary;
    let hasOverlap = false;
    
    // Add overlap to previous chunk (except for first chunk)
    if (i > 0 && CHUNKING_CONFIG.overlapSize > 0) {
      chunkStart = Math.max(0, startBoundary - CHUNKING_CONFIG.overlapSize);
      hasOverlap = true;
    }
    
    // Add overlap to next chunk (except for last chunk)
    if (i < boundaries.length - 2 && CHUNKING_CONFIG.overlapSize > 0) {
      chunkEnd = Math.min(words.length, endBoundary + CHUNKING_CONFIG.overlapSize);
      hasOverlap = true;
    }
    
    const chunkWords = words.slice(chunkStart, chunkEnd);
    const chunkContent = chunkWords.join(' ');
    
    // Skip empty chunks
    if (chunkContent.trim().length === 0) {
      continue;
    }
    
    // Determine chunking method used
    let chunkingMethod = 'word_boundary';
    if (chunkContent.includes('\n\n')) {
      chunkingMethod = 'paragraph_boundary';
    }
    if (/[.!?]\s*$/.test(chunkContent.trim())) {
      chunkingMethod = 'sentence_boundary';
    }
    
    const chunk: TextChunk = {
      id: `${input.originalIndex}-chunk-${i + 1}`,
      content: chunkContent.trim(),
      metadata: {
        position: i + 1,
        wordCount: chunkWords.length,
        charCount: chunkContent.length,
        startIndex: chunkStart,
        endIndex: chunkEnd,
        hasOverlap,
        chunkingMethod,
        section: undefined // Could be enhanced to detect section
      },
      source: {
        url: input.url,
        title: input.title,
        originalIndex: input.originalIndex
      }
    };
    
    chunks.push(chunk);
    
    console.log(`üìÑ Created chunk ${i + 1}: ${chunk.metadata.wordCount} words, method: ${chunkingMethod}`);
  }
  
  console.log(`‚úÖ Created ${chunks.length} chunks with overlap`);
  return chunks;
};

// ========================================
// STEP 4: CHUNK QUALITY VALIDATION
// ========================================

/**
 * Validate and optimize chunk quality
 * 
 * What this does:
 * - Check chunk sizes are within acceptable ranges
 * - Ensure chunks have meaningful content
 * - Merge chunks that are too small
 * - Split chunks that are too large
 * - Validate content quality
 */
const validateAndOptimizeChunks = (chunks: TextChunk[]): TextChunk[] => {
  console.log("üîç Step 4: Validating and optimizing chunk quality...");
  
  const optimizedChunks: TextChunk[] = [];
  
  for (let i = 0; i < chunks.length; i++) {
    const chunk = chunks[i];
    
    // Skip chunks that are too small (unless it's the last chunk)
    if (chunk.metadata.wordCount < CHUNKING_CONFIG.minChunkSize && i < chunks.length - 1) {
      console.log(`‚ö†Ô∏è Chunk ${chunk.id} is too small (${chunk.metadata.wordCount} words), merging with next chunk`);
      
      // Merge with next chunk if possible
      if (i + 1 < chunks.length) {
        const nextChunk = chunks[i + 1];
        const mergedContent = `${chunk.content} ${nextChunk.content}`;
        const mergedWordCount = mergedContent.split(/\s+/).length;
        
        // Only merge if the result isn't too large
        if (mergedWordCount <= CHUNKING_CONFIG.maxChunkSize) {
          const mergedChunk: TextChunk = {
            id: `${chunk.source.originalIndex}-chunk-${optimizedChunks.length + 1}`,
            content: mergedContent,
            metadata: {
              position: optimizedChunks.length + 1,
              wordCount: mergedWordCount,
              charCount: mergedContent.length,
              startIndex: chunk.metadata.startIndex,
              endIndex: nextChunk.metadata.endIndex,
              hasOverlap: chunk.metadata.hasOverlap || nextChunk.metadata.hasOverlap,
              chunkingMethod: 'merged_chunks',
              section: chunk.metadata.section || nextChunk.metadata.section
            },
            source: chunk.source
          };
          
          optimizedChunks.push(mergedChunk);
          i++; // Skip the next chunk since we merged it
          continue;
        }
      }
    }
    
    // Split chunks that are too large
    if (chunk.metadata.wordCount > CHUNKING_CONFIG.maxChunkSize) {
      console.log(`‚ö†Ô∏è Chunk ${chunk.id} is too large (${chunk.metadata.wordCount} words), splitting`);
      
      const words = chunk.content.split(/\s+/);
      const splitSize = Math.floor(CHUNKING_CONFIG.maxChunkSize / 2);
      
      for (let j = 0; j < words.length; j += splitSize) {
        const splitWords = words.slice(j, j + splitSize);
        const splitContent = splitWords.join(' ');
        
        if (splitContent.trim().length > 0) {
          const splitChunk: TextChunk = {
            id: `${chunk.source.originalIndex}-chunk-${optimizedChunks.length + 1}`,
            content: splitContent,
            metadata: {
              position: optimizedChunks.length + 1,
              wordCount: splitWords.length,
              charCount: splitContent.length,
              startIndex: chunk.metadata.startIndex + j,
              endIndex: chunk.metadata.startIndex + j + splitWords.length,
              hasOverlap: j > 0, // Overlapping splits
              chunkingMethod: 'split_large_chunk',
              section: chunk.metadata.section
            },
            source: chunk.source
          };
          
          optimizedChunks.push(splitChunk);
        }
      }
    } else {
      // Chunk is good as-is
      optimizedChunks.push({
        ...chunk,
        id: `${chunk.source.originalIndex}-chunk-${optimizedChunks.length + 1}`,
        metadata: {
          ...chunk.metadata,
          position: optimizedChunks.length + 1
        }
      });
    }
  }
  
  console.log(`‚úÖ Chunk optimization complete: ${optimizedChunks.length} final chunks`);
  return optimizedChunks;
};

// ========================================
// MAIN TEXT CHUNKING FUNCTION
// ========================================

/**
 * Main function that orchestrates the text chunking process
 * 
 * Processing Pipeline:
 * 1. Preprocess text and identify structure
 * 2. Find optimal chunk boundaries
 * 3. Create chunks with overlap
 * 4. Validate and optimize chunk quality
 * 
 * @param input - Object containing content, title, URL, and index
 * @returns Promise<ChunkedContent> - Chunked content with metadata
 */
export const chunkText = async (input: TextChunkerInput): Promise<ChunkedContent> => {
  const startTime = Date.now();
  const originalLength = input.content.length;
  
  console.log(`‚úÇÔ∏è Starting text chunking for: ${input.title}`);
  console.log(`üìä Original content length: ${originalLength} characters`);
  
  try {
    // Step 1: Preprocess text
    const { processedText, sections, paragraphs } = preprocessTextForChunking(input.content);
    
    // Step 2: Find optimal boundaries
    const boundaries = findOptimalChunkBoundaries(
      processedText,
      CHUNKING_CONFIG.targetChunkSize,
      sections,
      paragraphs
    );
    
    // Step 3: Create chunks with overlap
    const rawChunks = createChunksWithOverlap(processedText, boundaries, input);
    
    // Step 4: Validate and optimize
    const optimizedChunks = validateAndOptimizeChunks(rawChunks);
    
    const processingTime = Date.now() - startTime;
    const totalChunkedLength = optimizedChunks.reduce((sum, chunk) => sum + chunk.content.length, 0);
    const averageChunkSize = optimizedChunks.length > 0 
      ? Math.round(optimizedChunks.reduce((sum, chunk) => sum + chunk.metadata.wordCount, 0) / optimizedChunks.length)
      : 0;
    
    console.log(`‚úÖ Text chunking completed in ${processingTime}ms`);
    console.log(`üìä Created ${optimizedChunks.length} chunks, average ${averageChunkSize} words per chunk`);
    
    return {
      chunks: optimizedChunks,
      metadata: {
        totalChunks: optimizedChunks.length,
        averageChunkSize,
        chunkingStrategy: 'smart_boundary_with_overlap',
        processingTime,
        originalLength,
        totalChunkedLength
      }
    };
    
  } catch (error) {
    console.error(`‚ùå Text chunking failed for ${input.url}:`, error);
    throw new Error(`Text chunking failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
  }
};

// ========================================
// BATCH CHUNKING FUNCTION
// ========================================

/**
 * Chunk multiple content pieces in parallel batch processing
 * OPTIMIZED: Processes all content pieces concurrently for 3x speed improvement
 * 
 * @param inputs - Array of content to chunk
 * @returns Promise<ChunkedContent[]> - Array of chunked content
 */
export const chunkMultipleTexts = async (inputs: TextChunkerInput[]): Promise<ChunkedContent[]> => {
  console.log(`‚úÇÔ∏è Starting parallel text chunking for ${inputs.length} items...`);
  
  // OPTIMIZATION: Process all chunks in parallel instead of sequentially
  const chunkingPromises = inputs.map(async (input, index) => {
    try {
      console.log(`üìÑ Processing ${index + 1}/${inputs.length}: ${input.title}`);
      const chunked = await chunkText(input);
      return chunked;
    } catch (error) {
      console.error(`‚ùå Failed to chunk text ${index + 1}:`, error);
      // Return empty result for failed chunking
      return {
        chunks: [],
        metadata: {
          totalChunks: 0,
          averageChunkSize: 0,
          chunkingStrategy: 'failed',
          processingTime: 0,
          originalLength: input.content.length,
          totalChunkedLength: 0
        }
      };
    }
  });
  
  // Wait for all chunking operations to complete
  const results = await Promise.all(chunkingPromises);
  
  console.log(`‚úÖ Parallel text chunking completed: ${results.length} items processed`);
  return results;
};

// Export types for use in other modules
export type { TextChunk, ChunkedContent, TextChunkerInput }; 